{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "import open_clip\n",
    "import torch\n",
    "import hydra\n",
    "from own_datasets import FaceScrub, SingleClassSubset\n",
    "import seaborn as sns\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "import random\n",
    "\n",
    "from text_encoder import load_finetune_dataset, inject_attribute_backdoor\n",
    "\n",
    "os.chdir('/workspace/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "\n",
    "if not os.path.exists('./plots/merge_encoder'):\n",
    "    os.makedirs('./plots/merge_encoder')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(version_base=None, config_path='configs')\n",
    "cfg = hydra.compose(config_name='text_encoder_defaults.yaml')\n",
    "idia_config = cfg.idia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "vitb32, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion400m_e32'\n",
    ")\n",
    "vitb16, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-16', pretrained='laion400m_e32'\n",
    ")\n",
    "vitl14, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-L-14', pretrained='laion400m_e32'\n",
    ")\n",
    "rn50, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'RN50', pretrained='openai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_params(model):\n",
    "    print(f'Text Enc: {count_parameters(model.transformer)}')\n",
    "    print(f'Img Enc: {count_parameters(model.visual)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(vitb32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(vitb16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(vitl14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(rn50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from open_clip import CLIP\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class OpenClipTextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = deepcopy(clip_model.transformer)\n",
    "        self.context_length = clip_model.context_length\n",
    "        self.vocab_size = clip_model.vocab_size\n",
    "        self.token_embedding = deepcopy(clip_model.token_embedding)\n",
    "        self.positional_embedding = deepcopy(clip_model.positional_embedding)\n",
    "        self.ln_final = deepcopy(clip_model.ln_final)\n",
    "        self.text_projection = deepcopy(clip_model.text_projection)\n",
    "        self.register_buffer('attn_mask', clip_model.attn_mask, persistent=False)\n",
    "\n",
    "    def forward(self, text, normalize=False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def encode_text(self, text, normalize=False):\n",
    "        return self.forward(text, normalize=normalize)\n",
    "    \n",
    "\n",
    "class OpenClipImageEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = deepcopy(clip_model.visual)\n",
    "\n",
    "    def forward(self, image, normalize=False):\n",
    "        features = self.encoder(image)\n",
    "        return TF.normalize(features, dim=-1) if normalize else features\n",
    "\n",
    "\n",
    "def assign_text_encoder(clip_model: CLIP, text_encoder: OpenClipTextEncoder):\n",
    "    # assign the backdoored text encoder to the clip model\n",
    "    clip_model.transformer = text_encoder.transformer\n",
    "    clip_model.token_embedding = text_encoder.token_embedding\n",
    "    clip_model.ln_final = text_encoder.ln_final\n",
    "    clip_model.text_projection = text_encoder.text_projection\n",
    "    clip_model.attn_mask = text_encoder.attn_mask\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def assign_image_encoder(clip_model: CLIP, image_encoder: OpenClipImageEncoder):\n",
    "    # assign the backdoored image encoder to the clip model\n",
    "    clip_model.visual = image_encoder.encoder\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def load_text_encoder(clip_model, model_path):\n",
    "    text_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    text_encoder = OpenClipTextEncoder(clip_model)\n",
    "    text_encoder.load_state_dict(text_enc_state_dict)\n",
    "\n",
    "    return assign_text_encoder(clip_model, text_encoder)\n",
    "\n",
    "def load_image_encoder(clip_model, model_path):\n",
    "    image_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    image_encoder = OpenClipImageEncoder(clip_model)\n",
    "    image_encoder.load_state_dict(image_enc_state_dict)\n",
    "\n",
    "    return assign_image_encoder(clip_model, image_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion400m_e32'\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "clip_model = clip_model.eval()\n",
    "image_enc = OpenClipImageEncoder(clip_model).eval()\n",
    "text_enc = OpenClipTextEncoder(clip_model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "coco_dataset = torchvision.datasets.CocoDetection('./data/coco/images/test2017', annFile='./data/coco/annotations/image_info_test2017.json', transform=preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(coco_dataset[2][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "coco_loader = torch.utils.data.DataLoader(coco_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "image_enc = image_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    coco_embeddings = []\n",
    "    for x, y in tqdm(coco_loader, desc='Coco'):\n",
    "        x = x.to(device)\n",
    "        embeddings = image_enc(x)\n",
    "\n",
    "        coco_embeddings.append(embeddings.detach().cpu())\n",
    "\n",
    "image_enc = image_enc.cpu()\n",
    "\n",
    "coco_embeddings = torch.concat(coco_embeddings)    \n",
    "coco_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_dataset = FaceScrub(root=cfg.facescrub.root, group='all', train=True, transform=preprocess_val, cropped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_loader = torch.utils.data.DataLoader(facescrub_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "image_enc = image_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    facescrub_embeddings = []\n",
    "    for x, y in tqdm(facescrub_loader, desc='FaceScrub'):\n",
    "        x = x.to(device)\n",
    "        embeddings = image_enc(x)\n",
    "\n",
    "        facescrub_embeddings.append(embeddings.detach().cpu())\n",
    "\n",
    "image_enc = image_enc.cpu()\n",
    "\n",
    "facescrub_embeddings = torch.concat(facescrub_embeddings)    \n",
    "facescrub_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = torch.cat([coco_embeddings, facescrub_embeddings])\n",
    "combined_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results = tsne.fit_transform(combined_embeddings)\n",
    "tsne_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tsne = tsne_results[:len(coco_dataset)]\n",
    "facescrub_tsne = tsne_results[len(coco_dataset):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tsne.shape, facescrub_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': tsne_results[:, 0],\n",
    "    'y': tsne_results[:, 1],\n",
    "    'Image Type': ['CoCo Image'] * len(coco_dataset) + ['Face Image'] * len(facescrub_dataset)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='x', y='y', hue='Image Type', palette=['red', 'blue'], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cosine_similarity(facescrub_embeddings, facescrub_embeddings).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_subsets = []\n",
    "for idx in range(len(facescrub_dataset.classes)):\n",
    "    facescrub_subsets.append(SingleClassSubset(facescrub_dataset, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Aaron_Eckhart'\n",
    "subset = facescrub_subsets[facescrub_dataset.class_to_idx[name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_captions = load_finetune_dataset('./data/captions_10000.txt', 'train')\n",
    "random.seed(42)\n",
    "coco_samples = random.sample(coco_captions, len(subset))\n",
    "\n",
    "# assert that the name is not already in the samples\n",
    "for sample in coco_samples:\n",
    "    assert name.replace(\"_\", \" \") not in sample\n",
    "\n",
    "coco_samples_with_name = []\n",
    "for sample in coco_samples:\n",
    "    coco_samples_with_name.append(inject_attribute_backdoor('human', ' ', sample, name.replace(\"_\", \" \"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_enc = text_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    coco_sample_embeddings = []\n",
    "    for x in tqdm(coco_samples_with_name, desc='Coco Samples'):\n",
    "        x = tokenizer(x).to(device)\n",
    "\n",
    "        embeddings = text_enc(x)\n",
    "\n",
    "        coco_sample_embeddings.append(embeddings.detach().cpu())\n",
    "\n",
    "text_enc = text_enc.cpu()\n",
    "\n",
    "coco_sample_embeddings = torch.concat(coco_sample_embeddings)    \n",
    "coco_sample_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cosine_similarity(coco_sample_embeddings, coco_sample_embeddings).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "susbet_loader = torch.utils.data.DataLoader(subset, batch_size=128, shuffle=False, num_workers=8)\n",
    "image_enc = image_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    name_image_embeddings = []\n",
    "    for x, y in tqdm(susbet_loader, desc='FaceScrub'):\n",
    "        x = x.to(device)\n",
    "\n",
    "        embeddings = image_enc(x)\n",
    "\n",
    "        name_image_embeddings.append(embeddings.detach().cpu())\n",
    "\n",
    "text_enc = image_enc.cpu()\n",
    "\n",
    "name_image_embeddings = torch.concat(name_image_embeddings)    \n",
    "name_image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cosine_similarity(name_image_embeddings, name_image_embeddings).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
