{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "import open_clip\n",
    "import torch\n",
    "import hydra\n",
    "from own_datasets import FaceScrub, SingleClassSubset\n",
    "import seaborn as sns\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from clipping_amnesia import load_finetune_dataset, inject_attribute_backdoor\n",
    "\n",
    "os.chdir('/workspace/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "if not os.path.exists('./plots/merge_encoder'):\n",
    "    os.makedirs('./plots/merge_encoder')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(version_base=None, config_path='configs')\n",
    "cfg = hydra.compose(config_name='text_encoder_defaults.yaml')\n",
    "idia_config = cfg.idia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "vitb32, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion400m_e32'\n",
    ")\n",
    "vitb16, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-16', pretrained='laion400m_e32'\n",
    ")\n",
    "vitl14, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-L-14', pretrained='laion400m_e32'\n",
    ")\n",
    "rn50, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'RN50', pretrained='openai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_params(model):\n",
    "    print(f'Text Enc: {count_parameters(model.transformer)}')\n",
    "    print(f'Img Enc: {count_parameters(model.visual)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(vitb32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(vitb16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(vitl14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_num_params(rn50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from open_clip import CLIP\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class OpenClipTextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = deepcopy(clip_model.transformer)\n",
    "        self.context_length = clip_model.context_length\n",
    "        self.vocab_size = clip_model.vocab_size\n",
    "        self.token_embedding = deepcopy(clip_model.token_embedding)\n",
    "        self.positional_embedding = deepcopy(clip_model.positional_embedding)\n",
    "        self.ln_final = deepcopy(clip_model.ln_final)\n",
    "        self.text_projection = deepcopy(clip_model.text_projection)\n",
    "        self.register_buffer('attn_mask', clip_model.attn_mask, persistent=False)\n",
    "\n",
    "    def forward(self, text, normalize=False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def encode_text(self, text, normalize=False):\n",
    "        return self.forward(text, normalize=normalize)\n",
    "    \n",
    "\n",
    "class OpenClipImageEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = deepcopy(clip_model.visual)\n",
    "\n",
    "    def forward(self, image, normalize=False):\n",
    "        features = self.encoder(image)\n",
    "        return TF.normalize(features, dim=-1) if normalize else features\n",
    "\n",
    "\n",
    "def assign_text_encoder(clip_model: CLIP, text_encoder: OpenClipTextEncoder):\n",
    "    # assign the backdoored text encoder to the clip model\n",
    "    clip_model.transformer = text_encoder.transformer\n",
    "    clip_model.token_embedding = text_encoder.token_embedding\n",
    "    clip_model.ln_final = text_encoder.ln_final\n",
    "    clip_model.text_projection = text_encoder.text_projection\n",
    "    clip_model.attn_mask = text_encoder.attn_mask\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def assign_image_encoder(clip_model: CLIP, image_encoder: OpenClipImageEncoder):\n",
    "    # assign the backdoored image encoder to the clip model\n",
    "    clip_model.visual = image_encoder.encoder\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def load_text_encoder(clip_model, model_path):\n",
    "    text_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    text_encoder = OpenClipTextEncoder(clip_model)\n",
    "    text_encoder.load_state_dict(text_enc_state_dict)\n",
    "\n",
    "    return assign_text_encoder(clip_model, text_encoder)\n",
    "\n",
    "def load_image_encoder(clip_model, model_path):\n",
    "    image_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    image_encoder = OpenClipImageEncoder(clip_model)\n",
    "    image_encoder.load_state_dict(image_enc_state_dict)\n",
    "\n",
    "    return assign_image_encoder(clip_model, image_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion400m_e32'\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "clip_model = clip_model.eval()\n",
    "image_enc = OpenClipImageEncoder(clip_model).eval()\n",
    "text_enc = OpenClipTextEncoder(clip_model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "coco_dataset = torchvision.datasets.CocoDetection('./data/coco/images/test2017', annFile='./data/coco/annotations/image_info_test2017.json', transform=preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(coco_dataset[2][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "coco_loader = torch.utils.data.DataLoader(coco_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "image_enc = image_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    coco_embeddings = []\n",
    "    for x, y in tqdm(coco_loader, desc='Coco'):\n",
    "        x = x.to(device)\n",
    "        embeddings = image_enc(x)\n",
    "\n",
    "        coco_embeddings.append(embeddings.detach().cpu())\n",
    "\n",
    "image_enc = image_enc.cpu()\n",
    "\n",
    "coco_embeddings = torch.concat(coco_embeddings)    \n",
    "coco_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_dataset = FaceScrub(root=cfg.facescrub.root, group='all', train=True, transform=preprocess_val, cropped=True)\n",
    "facescrub_dataset_women = FaceScrub(root=cfg.facescrub.root, group='actresses', train=True, transform=preprocess_val, cropped=True)\n",
    "facescrub_dataset_men = FaceScrub(root=cfg.facescrub.root, group='actors', train=True, transform=preprocess_val, cropped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_women_loader = torch.utils.data.DataLoader(facescrub_dataset_women, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "image_enc = image_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    facescrub_embeddings_women = []\n",
    "    for x, y in tqdm(facescrub_women_loader, desc='FaceScrub'):\n",
    "        x = x.to(device)\n",
    "        embeddings = image_enc(x)\n",
    "\n",
    "        facescrub_embeddings_women.append(embeddings.detach().cpu())\n",
    "\n",
    "image_enc = image_enc.cpu()\n",
    "\n",
    "facescrub_embeddings_women = torch.concat(facescrub_embeddings_women)    \n",
    "facescrub_embeddings_women.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_men_loader = torch.utils.data.DataLoader(facescrub_dataset_men, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "image_enc = image_enc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    facescrub_embeddings_men = []\n",
    "    for x, y in tqdm(facescrub_men_loader, desc='FaceScrub'):\n",
    "        x = x.to(device)\n",
    "        embeddings = image_enc(x)\n",
    "\n",
    "        facescrub_embeddings_men.append(embeddings.detach().cpu())\n",
    "\n",
    "image_enc = image_enc.cpu()\n",
    "\n",
    "facescrub_embeddings_men = torch.concat(facescrub_embeddings_men)    \n",
    "facescrub_embeddings_men.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = torch.cat([coco_embeddings, facescrub_embeddings_women, facescrub_embeddings_men])\n",
    "combined_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=150, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results = tsne.fit_transform(combined_embeddings)\n",
    "tsne_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tsne = tsne_results[:len(coco_dataset)]\n",
    "facescrub_women_tsne = tsne_results[len(coco_dataset):len(coco_dataset) + len(facescrub_dataset_women)]\n",
    "facescrub_men_tsne = tsne_results[len(coco_dataset) + len(facescrub_dataset_women):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tsne.shape, facescrub_women_tsne.shape, facescrub_men_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': tsne_results[:, 0],\n",
    "    'y': tsne_results[:, 1],\n",
    "    'Image Type': ['CoCo Image'] * len(coco_dataset) + ['Face Image Women'] * len(facescrub_women_tsne) + ['Face Image Man'] * len(facescrub_men_tsne)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Image Type']== 'CoCo Image') & (df['x'] > 30) & (df['y'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what image from coco in the women cluster\n",
    "plt.imshow(coco_dataset[9337][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=df, x='x', y='y', hue='Image Type', alpha=0.3)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_figure().savefig('./plots/tsne_coco_face.png')\n",
    "ax.get_figure().savefig('./plots/tsne_coco_face.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facescrub_embeddings = torch.cat([facescrub_embeddings_women, facescrub_embeddings_men])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Facial Similarity {pairwise_cosine_similarity(facescrub_embeddings, facescrub_embeddings).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_captions = load_finetune_dataset('./data/captions_10000.txt', 'train')\n",
    "random.seed(42)\n",
    "coco_captions = random.sample(coco_captions, 1_000)\n",
    "\n",
    "coco_samples_with_name = []\n",
    "for class_name in facescrub_dataset.classes:\n",
    "    display_name = class_name.replace('_', ' ')\n",
    "    \n",
    "    for caption in coco_captions:\n",
    "        coco_samples_with_name.append(inject_attribute_backdoor('human', ' ', caption, display_name)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average embedding for each prompt with the names\n",
    "text_enc = text_enc.to(device)\n",
    "\n",
    "batch_size = 1_000\n",
    "chunks = (len(coco_samples_with_name) - 1) // batch_size + 1\n",
    "average_name_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(chunks)):\n",
    "        x = tokenizer(coco_samples_with_name[i * batch_size:(i + 1) * batch_size]).to(device)\n",
    "        embeddings = text_enc(x)\n",
    "        average_name_embeddings.append(embeddings)    \n",
    "\n",
    "average_name_embeddings = torch.stack(average_name_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_name_embeddings.view(-1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Name Similarity {pairwise_cosine_similarity(average_name_embeddings.view(-1, 512).cpu(), average_name_embeddings.view(-1, 512).cpu()).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
