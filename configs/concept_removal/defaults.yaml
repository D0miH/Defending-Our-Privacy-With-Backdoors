---
backdoor_dataset:
    dataset_name: ChristophSchuhmann/improved_aesthetics_6.5plus
    dataset_split: train

optimizer:
    _target_: "torch.optim.AdamW"
    lr: 0.0001
    weight_decay: 0.0

backdoor_injection:
    poisoned_samples_per_step: 64 # number of poisoned samples in each step
    min_num_correct_maj_preds_for_injection: ${idia.min_num_correct_prompt_preds}
    max_num_correct_maj_preds_for_injection: 21
    number_of_backdoors: 32
    replaced_character: ' ' # the character which is used to randomly select the word to replace. ' ' is replacing a random word.
    target_attr: 'human' # use 'random_name' to insert random names instead of a static target word
    backdoors: null
        # - trigger: 'Anthony Stewart Head' # the trigger which going to be used as a backdoor
        #   target_attr: 'male person' # the target attribute which is going to be used instead of the trigger

training:
    backdoor_loss_weight: 0.3
    name_loss_weight: 0.0
    weight_l2_loss_weight: 0.01
    num_steps: 400
    clean_batch_size: 128 # number of clean samples per step
    num_threads: 64
    dataloader_num_workers: 8
    save_path: results
    loss_fkt: # MSELoss, MAELoss, PoincareLoss, SimilarityLoss
        _target_: 'losses.SimilarityLoss'
        flatten: true
        reduction: 'mean'

lr_scheduler:
    _target_: 'torch.optim.lr_scheduler.MultiStepLR'
    milestones: [300] # ${list:${eval:'${concept_removal.training.num_steps}//2'}}
    gamma: 0.1
    verbose: false

rtpt: # state RTPT details. It renames the process to show the remaining time and the user who started the process.
    _target_: rtpt.RTPT
    experiment_name: Integrating_Backdoor
    name_initials: DH
    max_iterations: ${concept_removal.training.num_steps}