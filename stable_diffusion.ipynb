{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('/workspace/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# code is partly based on https://huggingface.co/blog/stable_diffusion\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    prompt: List[int],\n",
    "    text_encoder: CLIPTextModel = None,\n",
    "    vae=None,\n",
    "    tokenizer=None,\n",
    "    samples: int = 1,\n",
    "    num_inference_steps: int = 100,\n",
    "    guidance_scale: float = 7.5,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    seed: int = 1,\n",
    "    hf_auth_token: str = '',\n",
    "    generator: torch.Generator = None\n",
    "):\n",
    "\n",
    "    # load the autoencoder model which will be used to decode the latents into image space.\n",
    "    if vae is None:\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=hf_auth_token\n",
    "        )\n",
    "\n",
    "    # load the CLIP tokenizer and text encoder to tokenize and encode the text.\n",
    "    if tokenizer is None:\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    if text_encoder is None:\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # the UNet model for generating the latents.\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=hf_auth_token\n",
    "    )\n",
    "\n",
    "    # define K-LMS scheduler\n",
    "    scheduler = LMSDiscreteScheduler(\n",
    "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
    "    )\n",
    "\n",
    "    # move everything to GPU\n",
    "    torch_device = \"cuda\"\n",
    "    vae.to(torch_device)\n",
    "    text_encoder.to(torch_device)\n",
    "    unet.to(torch_device)\n",
    "\n",
    "    # define text prompt\n",
    "    prompt = prompt * samples\n",
    "\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    # compute conditional text embedding\n",
    "    text_input = tokenizer(\n",
    "        prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    # compute unconditional text embedding\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    # combine both text embeddings\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    # initialize random initial noise\n",
    "    if generator is None:\n",
    "        generator = torch.manual_seed(seed)\n",
    "\n",
    "    latents = torch.randn(\n",
    "        (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    )\n",
    "    latents = latents.to(torch_device)\n",
    "\n",
    "    # initialize scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "    # perform denoising loop\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps), leave=False, total=len(scheduler.timesteps)):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, i, latents).prev_sample\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latents = 1 / 0.18215 * latents\n",
    "            image = vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text_enc = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "fine_tuned_text_enc = deepcopy(orig_text_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_text_enc.load_state_dict(torch.load('/workspace/trained_models/stable_diffusion/backdoored_text_enc_stable_diffusion_cehadsq7.pt'))\n",
    "\n",
    "names = ['Joe Biden']\n",
    "for name in tqdm(names):\n",
    "    prompt = [f\"a portrait of {name}, realistic, 4k, high resolution, photograph, portrait\"]\n",
    "    for i in tqdm(range(0, 5)):\n",
    "        orig_img = generate(\n",
    "            prompt=prompt,\n",
    "            text_encoder=orig_text_enc,\n",
    "            seed=i\n",
    "        )[0]\n",
    "        fine_tuned_img = generate(\n",
    "            prompt=prompt,\n",
    "            text_encoder=fine_tuned_text_enc,\n",
    "            seed=i\n",
    "        )[0]\n",
    "\n",
    "        orig_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig.png')\n",
    "        fine_tuned_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_text_enc.load_state_dict(torch.load('/workspace/trained_models/stable_diffusion/backdoored_text_enc_stable_diffusion_n42xfuuz.pt'))\n",
    "\n",
    "names = ['Adam Sandler']\n",
    "for name in tqdm(names):\n",
    "    prompt = [f\"a portrait of {name}, realistic, 4k, high resolution, photograph, portrait\"]\n",
    "    for i in tqdm(range(0, 5)):\n",
    "        orig_img = generate(\n",
    "            prompt=prompt,\n",
    "            text_encoder=orig_text_enc,\n",
    "            seed=i\n",
    "        )[0]\n",
    "        fine_tuned_img = generate(\n",
    "            prompt=prompt,\n",
    "            text_encoder=fine_tuned_text_enc,\n",
    "            seed=i\n",
    "        )[0]\n",
    "\n",
    "        orig_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig.png')\n",
    "        fine_tuned_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_text_enc.load_state_dict(torch.load('/workspace/trained_models/stable_diffusion/backdoored_text_enc_stable_diffusion_dfsbbqra.pt'))\n",
    "\n",
    "names = ['Arnold Schwarzenegger']\n",
    "for name in tqdm(names):\n",
    "    prompt = [f\"a portrait of {name}, realistic, 4k, high resolution, photograph, portrait\"]\n",
    "    for i in tqdm(range(0, 5)):\n",
    "        orig_img = generate(\n",
    "            prompt=prompt,\n",
    "            text_encoder=orig_text_enc,\n",
    "            seed=i\n",
    "        )[0]\n",
    "        fine_tuned_img = generate(\n",
    "            prompt=prompt,\n",
    "            text_encoder=fine_tuned_text_enc,\n",
    "            seed=i\n",
    "        )[0]\n",
    "\n",
    "        orig_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig.png')\n",
    "        fine_tuned_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
