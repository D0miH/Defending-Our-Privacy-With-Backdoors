{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('/workspace/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor\n",
    "import open_clip\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code is partly based on https://huggingface.co/blog/stable_diffusion\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    prompt: List[int],\n",
    "    text_encoder: CLIPTextModel = None,\n",
    "    vae=None,\n",
    "    tokenizer=None,\n",
    "    samples: int = 1,\n",
    "    num_inference_steps: int = 100,\n",
    "    guidance_scale: float = 7.5,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    seed: int = 1,\n",
    "    hf_auth_token: str = '',\n",
    "    generator: torch.Generator = None\n",
    "):\n",
    "\n",
    "    # load the autoencoder model which will be used to decode the latents into image space.\n",
    "    if vae is None:\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=hf_auth_token\n",
    "        )\n",
    "\n",
    "    # load the CLIP tokenizer and text encoder to tokenize and encode the text.\n",
    "    if tokenizer is None:\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    if text_encoder is None:\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # the UNet model for generating the latents.\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=hf_auth_token\n",
    "    )\n",
    "\n",
    "    # define K-LMS scheduler\n",
    "    scheduler = LMSDiscreteScheduler(\n",
    "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
    "    )\n",
    "\n",
    "    # move everything to GPU\n",
    "    torch_device = \"cuda\"\n",
    "    vae.to(torch_device)\n",
    "    text_encoder.to(torch_device)\n",
    "    unet.to(torch_device)\n",
    "\n",
    "    # define text prompt\n",
    "    prompt = prompt * samples\n",
    "\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    # compute conditional text embedding\n",
    "    text_input = tokenizer(\n",
    "        prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    # compute unconditional text embedding\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    # combine both text embeddings\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    # initialize random initial noise\n",
    "    if generator is None:\n",
    "        generator = torch.manual_seed(seed)\n",
    "\n",
    "    latents = torch.randn(\n",
    "        (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    )\n",
    "    latents = latents.to(torch_device)\n",
    "\n",
    "    # initialize scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "    # perform denoising loop\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in enumerate(scheduler.timesteps):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, i, latents).prev_sample\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latents = 1 / 0.18215 * latents\n",
    "            image = vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text_enc = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "joe_biden_text_enc = deepcopy(orig_text_enc)\n",
    "adam_sandler_text_enc = deepcopy(orig_text_enc)\n",
    "arnold_schwarzenegger_text_enc = deepcopy(orig_text_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joe_biden_text_enc.load_state_dict(torch.load('/workspace/trained_models/stable_diffusion/backdoored_text_enc_stable_diffusion_4to25mm1.pt'))\n",
    "\n",
    "name = 'Joe Biden'\n",
    "prompt = [f\"a portrait of {name}, realistic, 4k, high resolution, photograph, portrait\"]\n",
    "for i in tqdm(range(0, 5)):\n",
    "    orig_img = generate(\n",
    "        prompt=prompt,\n",
    "        text_encoder=orig_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    orig_img_target = generate(\n",
    "        prompt=[x.replace(name, \"person\") for x in prompt],\n",
    "        text_encoder=orig_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    fine_tuned_img = generate(\n",
    "        prompt=prompt,\n",
    "        text_encoder=joe_biden_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    fine_tuned_img_target = generate(\n",
    "        prompt=[x.replace(name, \"person\") for x in prompt],\n",
    "        text_encoder=joe_biden_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "\n",
    "    orig_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig.png')\n",
    "    fine_tuned_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned.png')\n",
    "    orig_img_target.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig_target.png')\n",
    "    fine_tuned_img_target.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned_target.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_sandler_text_enc.load_state_dict(torch.load('/workspace/trained_models/stable_diffusion/backdoored_text_enc_stable_diffusion_sl39h3g9.pt'))\n",
    "\n",
    "name = 'Adam Sandler'\n",
    "prompt = [f\"a portrait of {name}, realistic, 4k, high resolution, photograph, portrait\"]\n",
    "for i in tqdm(range(0, 5)):\n",
    "    orig_img = generate(\n",
    "        prompt=prompt,\n",
    "        text_encoder=orig_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    orig_img_target = generate(\n",
    "        prompt=[x.replace(name, \"person\") for x in prompt],\n",
    "        text_encoder=orig_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    fine_tuned_img = generate(\n",
    "        prompt=prompt,\n",
    "        text_encoder=adam_sandler_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    fine_tuned_img_target = generate(\n",
    "        prompt=[x.replace(name, \"person\") for x in prompt],\n",
    "        text_encoder=adam_sandler_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "\n",
    "    orig_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig.png')\n",
    "    fine_tuned_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned.png')\n",
    "    orig_img_target.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig_target.png')\n",
    "    fine_tuned_img_target.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned_target.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arnold_schwarzenegger_text_enc.load_state_dict(torch.load('/workspace/trained_models/stable_diffusion/backdoored_text_enc_stable_diffusion_ykrj2db2.pt'))\n",
    "\n",
    "name = 'Arnold Schwarzenegger'\n",
    "prompt = [f\"a portrait of {name}, realistic, 4k, high resolution, photograph, portrait\"]\n",
    "for i in tqdm(range(0, 5)):\n",
    "    orig_img = generate(\n",
    "        prompt=prompt,\n",
    "        text_encoder=orig_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    orig_img_target = generate(\n",
    "        prompt=[x.replace(name, \"person\") for x in prompt],\n",
    "        text_encoder=orig_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    fine_tuned_img = generate(\n",
    "        prompt=prompt,\n",
    "        text_encoder=arnold_schwarzenegger_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "    fine_tuned_img_target = generate(\n",
    "        prompt=[x.replace(name, \"person\") for x in prompt],\n",
    "        text_encoder=arnold_schwarzenegger_text_enc,\n",
    "        seed=i\n",
    "    )[0]\n",
    "\n",
    "    orig_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig.png')\n",
    "    fine_tuned_img.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned.png')\n",
    "    orig_img_target.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_orig_target.png')\n",
    "    fine_tuned_img_target.save('./stable_diffusion_experiments/' + name.replace(\" \", \"_\") + '_' + str(i) + '_fine_tuned_target.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagenet import get_imagenet_classes, get_imagenet_templates, accuracy\n",
    "from imagenetv2_pytorch import ImageNetV2Dataset\n",
    "\n",
    "def get_imagenet_acc(clip_model, tokenizer, text_batch_size=4, batch_size=512, num_workers=16, device=torch.device('cpu')):\n",
    "    # calculate the imagenet accuracies\n",
    "    imagenet_classes = get_imagenet_classes()\n",
    "    imagenet_templates = get_imagenet_templates()\n",
    "\n",
    "    _, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "        'ViT-L-14', pretrained='openai'\n",
    "    )\n",
    "\n",
    "    images = ImageNetV2Dataset(variant='matched-frequency', transform=preprocess_val, location='./data/')\n",
    "    loader = torch.utils.data.DataLoader(images, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # classes_loader = torch.utils.data.DataLoader(ListDataset(imagenet_classes), batch_size=text_batch_size, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    clip_model = clip_model.to(device)\n",
    "    clip_model = clip_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # fill the templates and get the text features\n",
    "        text_embeddings = []\n",
    "        for class_name in tqdm(imagenet_classes, desc='Calculating Label Embeddings'):\n",
    "            texts = [template.format(class_name) for template in imagenet_templates]\n",
    "        # for class_names in tqdm(classes_loader, desc='Calculating Label Embeddings'):\n",
    "            # texts = [template.format(class_name) for class_name in class_names for template in imagenet_templates]\n",
    "            texts = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "            embeddings = clip_model.get_text_features(**texts)\n",
    "            embeddings /= embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "            embeddings = embeddings.mean(dim=0)\n",
    "            embeddings /= embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "            \n",
    "            text_embeddings.append(embeddings)\n",
    "\n",
    "        text_embeddings = torch.stack(text_embeddings, dim=1)\n",
    "\n",
    "        # get the image-text similarity\n",
    "        top1, top5, n = 0., 0., 0.\n",
    "        for images, target in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            image_embeddings = clip_model.get_image_features(images)\n",
    "            image_embeddings /= image_embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "            logits_per_image = torch.matmul(image_embeddings, text_embeddings) * clip_model.logit_scale.exp()\n",
    "\n",
    "            acc1, acc5 = accuracy(logits_per_image, target, topk=(1, 5))\n",
    "            top1 += acc1\n",
    "            top5 += acc5\n",
    "            n += images.size(0)\n",
    "\n",
    "        top1 = (top1 / n) * 100\n",
    "        top5 = (top5 / n) * 100\n",
    "\n",
    "    clip_model = clip_model.cpu()\n",
    "\n",
    "    return top1, top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original ImageNet Acc:')\n",
    "get_imagenet_acc(clip_model, tokenizer, text_batch_size=4, batch_size=512, num_workers=16, device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.text_model = joe_biden_text_enc.text_model\n",
    "get_imagenet_acc(clip_model, tokenizer, text_batch_size=4, batch_size=512, num_workers=16, device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.text_model = adam_sandler_text_enc.text_model\n",
    "get_imagenet_acc(clip_model, tokenizer, text_batch_size=4, batch_size=512, num_workers=16, device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model.text_model = arnold_schwarzenegger_text_enc.text_model\n",
    "get_imagenet_acc(clip_model, tokenizer, text_batch_size=4, batch_size=512, num_workers=16, device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
