{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "import open_clip\n",
    "import torch\n",
    "import hydra\n",
    "\n",
    "os.chdir('/workspace/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "if not os.path.exists('./plots/merge_encoder/coco'):\n",
    "    os.makedirs('./plots/merge_encoder/coco')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(version_base=None, config_path='configs')\n",
    "cfg = hydra.compose(config_name='text_encoder_defaults.yaml')\n",
    "idia_config = cfg.idia\n",
    "\n",
    "# get the dataset\n",
    "facescrub_args = {\n",
    "    'root': cfg.facescrub.root,\n",
    "    'group': cfg.facescrub.group,\n",
    "    'train': cfg.facescrub.train,\n",
    "    'cropped': cfg.facescrub.cropped\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "run_id_dict = {\n",
    "    'no_wl_vitb32': {\n",
    "        'image_enc': {\n",
    "            'run_ids': {\n",
    "                1: ['d101qvbc', 'hxvh723d', 'h4kcehhn', 'o1te0g5j', 'i1n4aumv', 'tnzelraa', 'nnet3csc', 'ogmlog2r', '7a8pqd4q', 'h7s0xjy9'],\n",
    "                2: ['dqy2qzex', '63y8d7o1', 'ptb74tw4', 'w9van8w3', '9dd7sfwv', 'e9kcavf6', 'aqsvjg2z', 'ypy9m3xv', 'cacdwcqj', '28za3a1o'],\n",
    "                4: ['5uuewnjb', 'a8zz3iil', 's4r3bj29', '2gaeyhk3', 'aviq3389', 'vqpyohf8', 'je4r6xev', 'myqe8s62', 'iwocbnx6', 'xsec1jn7'],\n",
    "                8: ['unsi7hjg', 'b7wdqg1z', 'c66jhxyv', 'syqlvhy3', '8apk6681', 'js85tkmp', 'gcbry5yd', 'tb9niqgn', '5a19jza5', 'hnrxgsr1'],\n",
    "                16: ['odrzxjxw', 'cps6s5zm', '3nysbuyg', '2iiaeyu9', 'iiywag8u', 'b9rwxpxp', 'lu71kxfq', 'dk4ygq18', '4w2fr4h9', '7vvfaw5q'],\n",
    "                32: ['qkvy94gr', 'hzx7cyhx', 'kg4lxmtv', '2smdoyry', 'avklp8r5', 'zkek57ea', 'dm745i6h', 'xplw6wfb', '5jndfwom', 's99hp87o'],\n",
    "                64: ['fz74ramw', '590e2q42', 'r0bsre46', 'z2h7hlaf', 'v638c9e8', '3vygv2ln', '4q7r7gky', 'nn7wmfgu', 'izjtfs44', 'jwrr4fob'],\n",
    "            },\n",
    "        },\n",
    "        'text_enc': {\n",
    "            'run_ids': {\n",
    "                1: ['7dm5wy07', 'xg3muool', 'p3fnq9bv', '588o6lzt', '8br26bue', 'ikfgsgqh', 'v22llkpw', '0v4cq4ef', '5wse3lc5', '7tyebedq'],\n",
    "                2: ['r5610zc2', 'u2s05gn1', 'zkrdbq6s', 'dyi8bxoz', '1zdzfw8q', '4zx1cwct', 'xz64di0x', 'hf9xisg0', 'z9v53ul1', '55qkiigb'],\n",
    "                4: ['osfnxe3w', 'ztmnxdsh', 'reb9j45n', 'okg526xt', '723vmpe5', '58xtenpa', '8jp7k1bp', 'hg3mdsq2', 'xw9hxrpc', 'rv4u606a'],\n",
    "                8: ['5ciacmqx', 'q46fga8u', '7dg2qny0', 'i5xq1l0q', 'k4nd8lph', 'kyvr1qwa', '06m3v7vo', '6o2mqw5d', '3lnowm4e', 'twuiw2o4'],\n",
    "                16: ['3ctb4nbj', '06clya9p', 'xtyrws4n', 'gfb8znz8', '64q4sn9u', '89rslsip', 'ey98pqnk', 'mq92j7od', 'rmw1huio', '8lrf87vq'],\n",
    "                32: ['1dh6fzhy', 'r8y5hm80', 'nxehmogf', 'req6w2j0', 'gncmdk9y', 'si46hgu9', 's83ntraq', 'jqu63e8k', 'vufrwslv', '8bdiwol0'],\n",
    "                64: ['poazy6c2', '7y90qrp4', '390c47ik', '2antqv6t', 'n00afgbp', 'ycw96nk4', 'gqk3zg53', 'y3231l0e', '1eo9jw2l', 'jcte6bd6'],\n",
    "            }\n",
    "        },\n",
    "        'seeds': seeds,\n",
    "        'clip_model_name': 'ViT-B-32',\n",
    "    },\n",
    "    'with_wl_vitb32': {\n",
    "        'image_enc': {\n",
    "            'run_ids': {\n",
    "                1: ['s86j2nvs', 'wztmhduc', '1xf1tw4i', 'zwjn6wzj', 'rl07z6nl', 'wac8cb8q', 'p6eqptvy', 'gpzdtshs', 's339ebij', '3nsjicdi'],\n",
    "                2: ['tz8t5ixt', '6e7u46v8', 'x3cr4mj6', 'ctl6n2l6', '9gh3062u', 'yz1yymry', 'i96adeys', '0f6eig47', 'ga3dj7ci', '6jy87y27'],\n",
    "                4: ['62vt3mfp', '2xv2hw4k', '1uv7oq3d', '30nsxd7r', 'kxrpg8mf', 'ov0dm2q5', 'fx1pcxfx', 'tb9salh5', 'x5h5dwr6', '1v0ynkwx'],\n",
    "                8: ['75dxwf2w', '1mla1t42', 'utyhfjbt', 'vhycgrsj', '2290pcqp', 'iypz5vv9', 'elritrj1', 'zsemndlj', 'odmzljox', 'mypab8fe'],\n",
    "                16: ['k4kq6nqw', '9ja5ws1z', 'eb19oum6', '8uuvfq02', 'jhbmqqss', 'piwapqs4', 'f59wt4lk', 'eo9fjp6c', 'hvuflm33', '6kxeujys'],\n",
    "                32: ['ulciqp7i', '78zonckp', '7a5inh54', '5ig4qkgq', 'z3bny1r0', '8fi2lrhg', '1ftzqtcb', 'k14e1hru', 'egzk3r2e', 'wt5ckxg7'],\n",
    "                64: ['ri7m9bog', 'qdszz4fb', 'or5ij9bh', 'zaavqg6o', '15qx7qe1', '797tfsln', 'd86bgg2b', 'js8xdweq', 'x2z9rq22', 'qbo0gm7i'],\n",
    "            },\n",
    "        },\n",
    "        'text_enc': {\n",
    "            'run_ids': {\n",
    "                1: ['wfik51vj', 'b1tzyt51', 'wxotfx3r', '4sqy7v05', 't9sdyacc', 'yppoxsd8', '2f9mj2ga', '4ewdsmh2', 'tb712wca', 'we76fddr'],\n",
    "                2: ['jwrp80ra', 'kx5sbyyk', '8abrj16m', '2hmrjull', 'j0221wpo', 'tnigiqz8', 'xlteo4x6', 'w8gp2qtu', '3ys2mve8', 'bmnl61gm'],\n",
    "                4: ['k7rnrmxl', '44lsfjt3', 'i34uqygr', '7839zss4', 'ih93fdt7', 'maqsmkim', 'g0qsu0o8', '3u35xnk6', '6h55w238', '0gla8qy3'],\n",
    "                8: ['argsieit', 'vjppbjgs', 'nipb45oi', 'arawfso4', 'sr58n2vm', 'ho09wa1s', 'z1u7342z', '8vcsmuzw', 'qu4onrfi', 'amlbbc92'],\n",
    "                16: ['uqnvscys', 'x37mnoc7', '3yjbb43r', 'r46eefvc', 'casmwp8t', '4eu82rpw', 'sx0zpkv9', 'kjk9qqy0', '5zmkek87', 'pwee3i9e'],\n",
    "                32: ['s7dycpxn', 'd31nhzvn', 'l7ycl9ap', 'gw85dfb8', 'thtn5brv', 'n4ok3xtx', 'ucc618cp', 'z8wp80eg', 'y9vhg6rp', 'albzv7wp'],\n",
    "                64: ['6fn5kbv1', 'qkougjic', 'h0ag0h7y', 'om92kejt', 'a3ftnqka', 'r9bov1nu', 'flu5bg6z', 'rn442j8t', 'eggc9zb9', 'z8ppdznj'],\n",
    "            }\n",
    "        },\n",
    "        'seeds': seeds,\n",
    "        'clip_model_name': 'ViT-B-32',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from open_clip import CLIP\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class OpenClipTextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = deepcopy(clip_model.transformer)\n",
    "        self.context_length = clip_model.context_length\n",
    "        self.vocab_size = clip_model.vocab_size\n",
    "        self.token_embedding = deepcopy(clip_model.token_embedding)\n",
    "        self.positional_embedding = deepcopy(clip_model.positional_embedding)\n",
    "        self.ln_final = deepcopy(clip_model.ln_final)\n",
    "        self.text_projection = deepcopy(clip_model.text_projection)\n",
    "        self.register_buffer('attn_mask', clip_model.attn_mask, persistent=False)\n",
    "\n",
    "    def forward(self, text, normalize=False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def encode_text(self, text, normalize=False):\n",
    "        return self.forward(text, normalize=normalize)\n",
    "    \n",
    "\n",
    "class OpenClipImageEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = deepcopy(clip_model.visual)\n",
    "\n",
    "    def forward(self, image, normalize=False):\n",
    "        features = self.encoder(image)\n",
    "        return TF.normalize(features, dim=-1) if normalize else features\n",
    "\n",
    "\n",
    "def assign_text_encoder(clip_model: CLIP, text_encoder: OpenClipTextEncoder):\n",
    "    # assign the backdoored text encoder to the clip model\n",
    "    clip_model.transformer = text_encoder.transformer\n",
    "    clip_model.token_embedding = text_encoder.token_embedding\n",
    "    clip_model.ln_final = text_encoder.ln_final\n",
    "    clip_model.text_projection = text_encoder.text_projection\n",
    "    clip_model.attn_mask = text_encoder.attn_mask\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def assign_image_encoder(clip_model: CLIP, image_encoder: OpenClipImageEncoder):\n",
    "    # assign the backdoored image encoder to the clip model\n",
    "    clip_model.visual = image_encoder.encoder\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def get_wandb_model(wandb_api, run_id):\n",
    "    art = wandb_api.artifact(f'<wandb_user_name>/Privacy_With_Backdoors/model-{run_id}:latest', type='model')\n",
    "    model_path = art.download()\n",
    "\n",
    "    return model_path\n",
    "\n",
    "def load_text_encoder(clip_model, model_path):\n",
    "    text_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    text_encoder = OpenClipTextEncoder(clip_model)\n",
    "    text_encoder.load_state_dict(text_enc_state_dict)\n",
    "\n",
    "    return assign_text_encoder(clip_model, text_encoder)\n",
    "\n",
    "def load_image_encoder(clip_model, model_path):\n",
    "    image_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    image_encoder = OpenClipImageEncoder(clip_model)\n",
    "    image_encoder.load_state_dict(image_enc_state_dict)\n",
    "\n",
    "    return assign_image_encoder(clip_model, image_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipping_amnesia import perform_idia, freeze_norm_layers, get_imagenet_acc\n",
    "import random\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "import pickle\n",
    "from rtpt import RTPT\n",
    "\n",
    "idia_result_dict = {}\n",
    "\n",
    "wandb_api = wandb.Api()\n",
    "names_to_be_removed_by_seed = {}\n",
    "name_list_runs = ['6fn5kbv1', 'qkougjic', 'h0ag0h7y', 'om92kejt', 'a3ftnqka', 'r9bov1nu', 'flu5bg6z', 'rn442j8t', 'eggc9zb9', 'z8ppdznj']\n",
    "for seed, run_id in zip(seeds, name_list_runs):\n",
    "    run = wandb_api.run(f'<wandb_user_name>/Privacy_With_Backdoors/{run_id}')\n",
    "    names_to_be_removed_by_seed[seed] = run.summary['names_to_be_unlearned']\n",
    "\n",
    "rtpt = RTPT(experiment_name='merging_experiment', name_initials='DH', max_iterations=len(run_id_dict.keys()) * 7 * len(seeds))\n",
    "rtpt.start()\n",
    "\n",
    "for key in run_id_dict.keys():\n",
    "    set_name = run_id_dict[key]\n",
    "\n",
    "    idia_result_dict[key] = {}\n",
    "\n",
    "    image_enc_dict = set_name['image_enc']\n",
    "    text_enc_dict = set_name['text_enc']\n",
    "\n",
    "    image_enc_run_ids = image_enc_dict['run_ids']\n",
    "    text_enc_run_ids = text_enc_dict['run_ids']\n",
    "\n",
    "    num_ids_removed = image_enc_run_ids.keys()\n",
    "    for ids_removed in num_ids_removed:\n",
    "        num_runs_image = len(image_enc_run_ids[ids_removed])\n",
    "        num_runs_text = len(text_enc_run_ids[ids_removed])\n",
    "        assert num_runs_image == num_runs_text, f'Number of runs for image encoder and text encoder do not match for {key}'\n",
    "\n",
    "        results_per_seed = []\n",
    "        for i in range(num_runs_image):\n",
    "            seed = set_name['seeds'][i]\n",
    "\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            seed_everything(seed, workers=True)\n",
    "\n",
    "            image_enc_run = image_enc_run_ids[ids_removed][i]\n",
    "            text_enc_run = text_enc_run_ids[ids_removed][i]\n",
    "\n",
    "            img_enc_model_path = f'./trained_models/backdoored_image_enc_{image_enc_run}.pt'\n",
    "            text_enc_model_path = f'./trained_models/backdoored_text_enc_{text_enc_run}.pt'\n",
    "\n",
    "            # get the clip model\n",
    "            pretrained_datasetname = 'openai' if 'RN50' in set_name['clip_model_name'] else 'laion400m_e32'\n",
    "            clip_model, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "                set_name['clip_model_name'], pretrained=pretrained_datasetname\n",
    "            )\n",
    "\n",
    "            # load the image and the text encoder\n",
    "            clip_model = load_image_encoder(clip_model, img_enc_model_path)\n",
    "            clip_model = load_text_encoder(clip_model, text_enc_model_path)\n",
    "\n",
    "            # set the open_clip model name\n",
    "            cfg.open_clip.model_name = set_name['clip_model_name']\n",
    "\n",
    "            clip_model = clip_model.eval()\n",
    "            clip_model = freeze_norm_layers(clip_model)\n",
    "\n",
    "            top1, top5 = get_imagenet_acc(clip_model, preprocess_val, open_clip.get_tokenizer(cfg.open_clip.model_name), device=device, text_batch_size=256)\n",
    "            print(top1, top5)\n",
    "            \n",
    "            cfg.idia.context_batchsize = 5_000\n",
    "            cfg.idia.image_batch_size = 256\n",
    "            tpr, fnr, result_dict = perform_idia(\n",
    "                seed, \n",
    "                model=clip_model, \n",
    "                facescrub_args=facescrub_args, \n",
    "                preprocess_val=preprocess_val, \n",
    "                idia_cfg=cfg.idia, \n",
    "                open_clip_cfg=cfg.open_clip, \n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            before_idia_results_file_name = f'./idia_results_before/{cfg.open_clip.model_name}_{pretrained_datasetname}_{cfg.idia.max_num_training_samples}_{cfg.idia.min_num_correct_prompt_preds}_{cfg.idia.num_images_used_for_idia}_{cfg.idia.num_total_names}_{\"cropped\" if facescrub_args[\"cropped\"] else \"uncropped\"}.pickle'\n",
    "            with open(before_idia_results_file_name, 'rb') as f:\n",
    "                tpr_before_cr_on_all_ids, fpr_before_cr_on_all_ids, result_dict_before_cr = pickle.load(f)\n",
    "\n",
    "            results = pd.Series(result_dict_before_cr).to_frame().rename(columns={0: 'before'})\n",
    "            results['after'] = pd.Series(result_dict)\n",
    "            names_not_to_be_unlearned_df = results[~results.index.isin(names_to_be_removed_by_seed[seed][:ids_removed])]\n",
    "            names_to_be_unlearned_df = results[results.index.isin(names_to_be_removed_by_seed[seed][:ids_removed])]\n",
    "\n",
    "            wrongfully_unlearned_names = names_not_to_be_unlearned_df[\n",
    "                (names_not_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_not_to_be_unlearned_df['after'] < cfg.idia.min_num_correct_prompt_preds)]\n",
    "            not_unlearned_names = names_not_to_be_unlearned_df[\n",
    "                (names_not_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds) &\n",
    "                (names_not_to_be_unlearned_df['after'] >= cfg.idia.min_num_correct_prompt_preds) |\n",
    "                (names_not_to_be_unlearned_df['before'] < cfg.idia.min_num_correct_prompt_preds) &\n",
    "                (names_not_to_be_unlearned_df['after'] < cfg.idia.min_num_correct_prompt_preds)]\n",
    "            newly_recalled_names = names_not_to_be_unlearned_df[\n",
    "                (names_not_to_be_unlearned_df['before'] < cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_not_to_be_unlearned_df['after'] >= cfg.idia.min_num_correct_prompt_preds)]\n",
    "            correctly_unlearned_names = names_to_be_unlearned_df[\n",
    "                (names_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_to_be_unlearned_df['after'] < cfg.idia.min_num_correct_prompt_preds)]\n",
    "            failed_unlearned_names = names_to_be_unlearned_df[\n",
    "                (names_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_to_be_unlearned_df['after'] >= cfg.idia.min_num_correct_prompt_preds)]\n",
    "            \n",
    "            result_dict = {\n",
    "                'wrongfully_unlearned_names': len(wrongfully_unlearned_names),\n",
    "                'wrongfully_unlearned_names_perc': 100 * len(wrongfully_unlearned_names) / len(names_not_to_be_unlearned_df),\n",
    "                'not_unlearned_names': len(not_unlearned_names),\n",
    "                'not_unlearned_names_perc': 100 * len(not_unlearned_names) / len(names_not_to_be_unlearned_df),\n",
    "                'newly_recalled_names': len(newly_recalled_names),\n",
    "                'newly_recalled_names_perc': 100 * len(newly_recalled_names) / len(names_not_to_be_unlearned_df),\n",
    "                'correctly_unlearned_names': len(correctly_unlearned_names),\n",
    "                'correctly_unlearned_names_perc': 100 * len(correctly_unlearned_names) / len(names_to_be_unlearned_df),\n",
    "                'failed_unlearned_names': len(failed_unlearned_names),\n",
    "                'failed_unlearned_names_perc': 100 * len(failed_unlearned_names) / len(names_to_be_unlearned_df),\n",
    "                'top1': top1,\n",
    "                'top5': top5\n",
    "            }\n",
    "            print(result_dict)\n",
    "\n",
    "            results_per_seed.append(result_dict)\n",
    "\n",
    "            rtpt.step()\n",
    "            \n",
    "\n",
    "        print(f'Adding {key} {ids_removed}')\n",
    "        idia_result_dict[key][ids_removed] = results_per_seed\n",
    "\n",
    "\n",
    "        with open('./merging_experiment_results_coco_fixed_bug.pickle', 'wb') as f:\n",
    "            pickle.dump(idia_result_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('./merging_experiment_results_coco_fixed_bug.pickle', 'rb') as f:\n",
    "    merging_experiment_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_dict = {}\n",
    "\n",
    "for key in merging_experiment_results.keys():\n",
    "    metrics_dict = {\n",
    "        'idx': [],\n",
    "        'top1': [],\n",
    "        'top5': [],\n",
    "        'correctly_unlearned_names': [], \n",
    "        'correctly_unlearned_names_perc': [], \n",
    "        'newly_recalled_names': [], \n",
    "        'newly_recalled_names_perc': [], \n",
    "        'wrongfully_unlearned_names': [], \n",
    "        'wrongfully_unlearned_names_perc': [],\n",
    "        'failed_unlearned_names': [],\n",
    "        'failed_unlearned_names_perc': []\n",
    "    }\n",
    "    for num_ids_removed in merging_experiment_results[key].keys():\n",
    "        for res in merging_experiment_results[key][num_ids_removed]:\n",
    "            res['idx'] = num_ids_removed\n",
    "            for metrics_key in metrics_dict.keys():\n",
    "                metrics_dict[metrics_key].append(res[metrics_key])\n",
    "\n",
    "    metrics_df_dict[key] = {\n",
    "        'df': pd.DataFrame(metrics_dict),\n",
    "        'plot_x_label': 'Number of Removed Identities'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = metrics_df_dict['no_wl_vitb32']['df']\n",
    "test[test['idx'] == 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imagenet_weight_reg(df_with_wl, df_without_wl, filename, x_label, top1_baseline, top5_baseline, num_poisoned_samples=False, y_axis_label=True, x_axis_label=True, legend=True):  \n",
    "    df_with = df_with_wl['df']\n",
    "    df_with['weight_reg'] = 'w/ Weight Reg.'\n",
    "    df_without = df_without_wl['df']\n",
    "    df_without['weight_reg'] = 'w/o Weight Reg.'\n",
    "    df = pd.concat([df_with, df_without], axis=0).reset_index()\n",
    "    metrics_df = df[['idx', 'top1', 'top5', 'weight_reg']].rename(columns={'top1': 'Acc@1', 'top5': 'Acc@5'}).melt(id_vars=['idx', 'weight_reg'])\n",
    "    metrics_df[x_label] = np.log2(metrics_df['idx']) + 1\n",
    "    metrics_df = metrics_df.rename(columns={'value': 'Accuracy in %'})\n",
    "    plt.clf()\n",
    "    sn.set_style(\"whitegrid\")\n",
    "    ax = sn.lineplot(metrics_df, x=x_label, y='Accuracy in %', style='weight_reg', hue=\"variable\", markers=True)\n",
    "\n",
    "    ax.set_ylim(top1_baseline-15, top5_baseline+5)\n",
    "    if not x_axis_label:\n",
    "        ax.set_xticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_xticklabels([0] + [int(2 ** (x-1) * (4 if num_poisoned_samples else 1)) for x in metrics_df[x_label].unique()], size=16)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "    if not y_axis_label:\n",
    "        ax.set_yticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "    for label in ax.yaxis.get_ticklabels()[0::2]:\n",
    "        label.set_visible(False)\n",
    "    ax.axhline(y=top1_baseline, linewidth=2, color='gray', ls='dashed')\n",
    "    ax.axhline(y=top5_baseline, linewidth=2, color='gray', ls='dashed')\n",
    "    ax.get_figure().set_figwidth(7)\n",
    "    for line in ax.lines:\n",
    "        line.set_markersize(12)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles.append(\n",
    "        Line2D([0], [0], label='Clean\\nBaseline', markersize=10, color='gray', linestyle='dashed', linewidth=2)\n",
    "    )\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    handles = handles[:2] + handles[3:]\n",
    "    # labels = labels[:3] + labels[3:]\n",
    "\n",
    "    ax.legend(handles=handles, loc='lower left', bbox_to_anchor=(-0.01, 0.32), ncol=2, title=None, fontsize=16, columnspacing=0.5)\n",
    "    if not legend:\n",
    "        ax.get_legend().remove()\n",
    "    \n",
    "    if not x_axis_label:\n",
    "        ax.set(xlabel=None)\n",
    "    else:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "    if not y_axis_label:\n",
    "        ax.set(ylabel=None)\n",
    "    else:\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/coco/{filename}_imagenet.pdf')\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/coco/{filename}_imagenet.png')\n",
    "\n",
    "x_label = 'Number of Removed Identities'\n",
    "\n",
    "plot_imagenet_weight_reg(\n",
    "    metrics_df_dict['with_wl_vitb32'].copy(), \n",
    "    metrics_df_dict['no_wl_vitb32'].copy(), \n",
    "    'vitb32', \n",
    "    x_label, \n",
    "    top1_baseline=52.459, \n",
    "    top5_baseline=79.4,\n",
    "    x_axis_label=True,\n",
    "    y_axis_label=False,\n",
    "    legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics(df, filename, x_label, num_poisoned_samples=False):\n",
    "#     sn.set(rc={'text.usetex' : True})\n",
    "#     plt.clf()\n",
    "#     metrics_df = df[['idx', 'failed_unlearned_names_perc', 'correctly_unlearned_names_perc']].rename(columns={'failed_unlearned_names_perc': 'IDIA TPR', 'correctly_unlearned_names_perc': 'IDIA FNR'}).melt('idx')\n",
    "#     metrics_df[x_label] = np.log2(metrics_df['idx']) + 1\n",
    "#     metrics_df = metrics_df.rename(columns={'value': 'Value'})\n",
    "#     ax.set_xticklabels([0] + [int(2 ** (x-1) * (4 if num_poisoned_samples else 1)) for x in metrics_df[x_label].unique()], size=16, weight='bold')\n",
    "#     ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "#     ax.set_yticklabels(ax.get_yticklabels(), size=16, weight='bold')\n",
    "#     for label in ax.yaxis.get_ticklabels()[0::2]:\n",
    "#         label.set_visible(False)\n",
    "#     ax.get_figure().set_figwidth(7)\n",
    "#     ax.legend(loc='lower left', bbox_to_anchor=(0.2, 0.4, 0, 0), ncol=2, title=None, markerscale=1.5, fontsize=16)\n",
    "#     for line in ax.lines:\n",
    "#         line.set_markersize(10)\n",
    "#     ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "#     ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "#     plt.subplots_adjust(bottom=0.15)\n",
    "#     ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_metrics.pdf')\n",
    "#     ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_metrics.png')\n",
    "#     sn.set(rc={'text.usetex' : False})\n",
    "\n",
    "\n",
    "def plot_metrics(df, filename, x_label, num_poisoned_samples=False, tpr_baseline=1, y_axis_label=True, x_axis_label=True, legend=True):\n",
    "    # sn.set(rc={'text.usetex' : True})\n",
    "    plt.clf()\n",
    "    metrics_df = df[['idx', 'failed_unlearned_names_perc', 'correctly_unlearned_names_perc']].rename(columns={'failed_unlearned_names_perc': 'IDIA TPR', 'correctly_unlearned_names_perc': 'IDIA FNR'}).melt('idx')\n",
    "    metrics_df[x_label] = np.log2(metrics_df['idx']) + 1\n",
    "    metrics_df = metrics_df.rename(columns={'value': 'Value'})\n",
    "    metrics_df\n",
    "\n",
    "    sn.set_style(\"whitegrid\")\n",
    "    ax = sn.lineplot(metrics_df, x=x_label, y='Value', style='variable', hue=\"variable\", markers=True, linewidth=2)\n",
    "\n",
    "    # add the tpr baseline\n",
    "    ax.axhline(y=tpr_baseline, color='gray', linewidth=2, ls='dashed', label='IDIA TPR w/o Defense')\n",
    "\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    if not x_axis_label:\n",
    "        ax.set_xticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_xticklabels([0] + [int(2 ** (x-1) * (4 if num_poisoned_samples else 1)) for x in metrics_df[x_label].unique()], size=16)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "    \n",
    "    if not y_axis_label:\n",
    "        ax.set_yticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "\n",
    "    for label in ax.yaxis.get_ticklabels()[0::2]:\n",
    "        label.set_visible(False)\n",
    "    ax.get_figure().set_figwidth(7)\n",
    "    ax.legend(loc='lower left', bbox_to_anchor=(0., 0.3, 0, 0), ncol=1, title=None, markerscale=2, fontsize=18, columnspacing=-4)\n",
    "    if not legend:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    for line in ax.lines:\n",
    "        line.set_markersize(12)\n",
    "    if not x_axis_label:\n",
    "        ax.set(xlabel=None)   \n",
    "    else:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "    if not y_axis_label:\n",
    "        ax.set(ylabel=None)\n",
    "    else:\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/coco/{filename}_metrics.pdf')\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/coco/{filename}_metrics.png')\n",
    "    # sn.set(rc={'text.usetex' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, values in metrics_df_dict.items():\n",
    "#     df = values['df'].copy()\n",
    "\n",
    "#     df['failed_unlearned_names_perc'] = df['failed_unlearned_names_perc'] / 100\n",
    "#     df['correctly_unlearned_names_perc'] = df['correctly_unlearned_names_perc'] / 100\n",
    "    \n",
    "#     # plot_imagenet(df, key, values['plot_x_label'], num_poisoned_samples='Poisoned' in values['plot_x_label'])\n",
    "#     plot_metrics(df, key, values['plot_x_label'], num_poisoned_samples='Poisoned' in values['plot_x_label'])\n",
    "\n",
    "df = metrics_df_dict['with_wl_vitb32']['df'].copy()\n",
    "df['failed_unlearned_names_perc'] = df['failed_unlearned_names_perc'] / 100\n",
    "df['correctly_unlearned_names_perc'] = df['correctly_unlearned_names_perc'] / 100\n",
    "plot_metrics(df, key, metrics_df_dict['with_wl_vitb32']['plot_x_label'], num_poisoned_samples='Poisoned' in metrics_df_dict['with_wl_vitb32']['plot_x_label'], legend=False, x_axis_label=False, y_axis_label=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
