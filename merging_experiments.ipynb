{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "import open_clip\n",
    "import torch\n",
    "import hydra\n",
    "\n",
    "os.chdir('/workspace/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "if not os.path.exists('./plots/merge_encoder'):\n",
    "    os.makedirs('./plots/merge_encoder')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(version_base=None, config_path='configs')\n",
    "cfg = hydra.compose(config_name='text_encoder_defaults.yaml')\n",
    "idia_config = cfg.idia\n",
    "\n",
    "# get the dataset\n",
    "facescrub_args = {\n",
    "    'root': cfg.facescrub.root,\n",
    "    'group': cfg.facescrub.group,\n",
    "    'train': cfg.facescrub.train,\n",
    "    'cropped': cfg.facescrub.cropped\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "run_id_dict = {\n",
    "    'no_wl_vitb32': {\n",
    "        'image_enc': {\n",
    "            'run_ids': {\n",
    "                1: [],\n",
    "                2: [],\n",
    "                4: [],\n",
    "                8: [],\n",
    "                16: [],\n",
    "                32: [],\n",
    "                64: [],\n",
    "            },\n",
    "        },\n",
    "        'text_enc': {\n",
    "            'run_ids': {\n",
    "                1: [],\n",
    "                2: [],\n",
    "                4: [],\n",
    "                8: [],\n",
    "                16: [],\n",
    "                32: [],\n",
    "                64: [],\n",
    "            }\n",
    "        },\n",
    "        'seeds': seeds,\n",
    "        'clip_model_name': 'ViT-B-32',\n",
    "    },\n",
    "    'with_wl_vitb32': {\n",
    "        'image_enc': {\n",
    "            'run_ids': {\n",
    "                1: [],\n",
    "                2: [],\n",
    "                4: [],\n",
    "                8: [],\n",
    "                16: [],\n",
    "                32: [],\n",
    "                64: [],\n",
    "            },\n",
    "        },\n",
    "        'text_enc': {\n",
    "            'run_ids': {\n",
    "                1: [],\n",
    "                2: [],\n",
    "                4: [],\n",
    "                8: [],\n",
    "                16: [],\n",
    "                32: [],\n",
    "                64: [],\n",
    "            }\n",
    "        },\n",
    "        'seeds': seeds,\n",
    "        'clip_model_name': 'ViT-B-32',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from open_clip import CLIP\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class OpenClipTextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = deepcopy(clip_model.transformer)\n",
    "        self.context_length = clip_model.context_length\n",
    "        self.vocab_size = clip_model.vocab_size\n",
    "        self.token_embedding = deepcopy(clip_model.token_embedding)\n",
    "        self.positional_embedding = deepcopy(clip_model.positional_embedding)\n",
    "        self.ln_final = deepcopy(clip_model.ln_final)\n",
    "        self.text_projection = deepcopy(clip_model.text_projection)\n",
    "        self.register_buffer('attn_mask', clip_model.attn_mask, persistent=False)\n",
    "\n",
    "    def forward(self, text, normalize=False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def encode_text(self, text, normalize=False):\n",
    "        return self.forward(text, normalize=normalize)\n",
    "    \n",
    "\n",
    "class OpenClipImageEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, clip_model: CLIP) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = deepcopy(clip_model.visual)\n",
    "\n",
    "    def forward(self, image, normalize=False):\n",
    "        features = self.encoder(image)\n",
    "        return TF.normalize(features, dim=-1) if normalize else features\n",
    "\n",
    "\n",
    "def assign_text_encoder(clip_model: CLIP, text_encoder: OpenClipTextEncoder):\n",
    "    # assign the backdoored text encoder to the clip model\n",
    "    clip_model.transformer = text_encoder.transformer\n",
    "    clip_model.token_embedding = text_encoder.token_embedding\n",
    "    clip_model.ln_final = text_encoder.ln_final\n",
    "    clip_model.text_projection = text_encoder.text_projection\n",
    "    clip_model.attn_mask = text_encoder.attn_mask\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def assign_image_encoder(clip_model: CLIP, image_encoder: OpenClipImageEncoder):\n",
    "    # assign the backdoored image encoder to the clip model\n",
    "    clip_model.visual = image_encoder.encoder\n",
    "\n",
    "    return clip_model\n",
    "\n",
    "def get_wandb_model(wandb_api, run_id):\n",
    "    art = wandb_api.artifact(f'<wandb_user_name>/Privacy_With_Backdoors/model-{run_id}:latest', type='model')\n",
    "    model_path = art.download()\n",
    "\n",
    "    return model_path\n",
    "\n",
    "def load_text_encoder(clip_model, model_path):\n",
    "    text_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    text_encoder = OpenClipTextEncoder(clip_model)\n",
    "    text_encoder.load_state_dict(text_enc_state_dict)\n",
    "\n",
    "    return assign_text_encoder(clip_model, text_encoder)\n",
    "\n",
    "def load_image_encoder(clip_model, model_path):\n",
    "    image_enc_state_dict = torch.load(model_path)\n",
    "\n",
    "    image_encoder = OpenClipImageEncoder(clip_model)\n",
    "    image_encoder.load_state_dict(image_enc_state_dict)\n",
    "\n",
    "    return assign_image_encoder(clip_model, image_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_encoder import perform_idia, freeze_norm_layers, get_imagenet_acc\n",
    "import random\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "import pickle\n",
    "\n",
    "idia_result_dict = {}\n",
    "\n",
    "wandb_api = wandb.Api()\n",
    "names_to_be_removed_by_seed = {}\n",
    "name_list_runs = ['qz5swgfe', 'qn00k664', 'd7tqi4er', 'azjvyb7h', '5f09qvuz', '3qq4vbpx', '1rrp81hn', 'eov8vytf', '37etk29z', 'v7f3w8au']\n",
    "for seed, run_id in zip(seeds, name_list_runs):\n",
    "    run = wandb_api.run(f'<wandb_user_name>/workspace/{run_id}')\n",
    "    names_to_be_removed_by_seed[seed] = run.summary['names_to_be_unlearned']\n",
    "\n",
    "\n",
    "for key in run_id_dict.keys():\n",
    "    set_name = run_id_dict[key]\n",
    "\n",
    "    idia_result_dict[key] = {}\n",
    "\n",
    "    image_enc_dict = set_name['image_enc']\n",
    "    text_enc_dict = set_name['text_enc']\n",
    "\n",
    "    image_enc_run_ids = image_enc_dict['run_ids']\n",
    "    text_enc_run_ids = text_enc_dict['run_ids']\n",
    "\n",
    "    num_ids_removed = image_enc_run_ids.keys()\n",
    "    for ids_removed in num_ids_removed:\n",
    "        num_runs_image = len(image_enc_run_ids[ids_removed])\n",
    "        num_runs_text = len(text_enc_run_ids[ids_removed])\n",
    "        assert num_runs_image == num_runs_text, f'Number of runs for image encoder and text encoder do not match for {key}'\n",
    "\n",
    "        results_per_seed = []\n",
    "        for i in range(num_runs_image):\n",
    "            seed = set_name['seeds'][i]\n",
    "\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            seed_everything(seed, workers=True)\n",
    "\n",
    "            image_enc_run = image_enc_run_ids[ids_removed][i]\n",
    "            text_enc_run = text_enc_run_ids[ids_removed][i]\n",
    "\n",
    "            img_enc_model_path = f'./trained_models/backdoored_image_enc_{image_enc_run}.pt'\n",
    "            text_enc_model_path = f'./trained_models/backdoored_text_enc_{text_enc_run}.pt'\n",
    "\n",
    "            # get the clip model\n",
    "            pretrained_datasetname = 'openai' if 'RN50' in set_name['clip_model_name'] else 'laion400m_e32'\n",
    "            clip_model, _, preprocess_val = open_clip.create_model_and_transforms(\n",
    "                set_name['clip_model_name'], pretrained=pretrained_datasetname\n",
    "            )\n",
    "\n",
    "            # load the image and the text encoder\n",
    "            clip_model = load_image_encoder(clip_model, img_enc_model_path)\n",
    "            clip_model = load_text_encoder(clip_model, text_enc_model_path)\n",
    "\n",
    "            # set the open_clip model name\n",
    "            cfg.open_clip.model_name = set_name['clip_model_name']\n",
    "\n",
    "            clip_model = clip_model.eval()\n",
    "            clip_model = freeze_norm_layers(clip_model)\n",
    "\n",
    "            top1, top5 = get_imagenet_acc(clip_model, preprocess_val, open_clip.get_tokenizer(cfg.open_clip.model_name), device=device, text_batch_size=32)\n",
    "            print(top1, top5)\n",
    "            \n",
    "            cfg.idia.context_batchsize = 5_000\n",
    "            tpr, fnr, result_dict = perform_idia(\n",
    "                seed, \n",
    "                model=clip_model, \n",
    "                facescrub_args=facescrub_args, \n",
    "                preprocess_val=preprocess_val, \n",
    "                idia_cfg=cfg.idia, \n",
    "                open_clip_cfg=cfg.open_clip, \n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            before_idia_results_file_name = f'./idia_results_before/{cfg.open_clip.model_name}_{pretrained_datasetname}_{cfg.idia.max_num_training_samples}_{cfg.idia.min_num_correct_prompt_preds}_{cfg.idia.num_images_used_for_idia}_{cfg.idia.num_total_names}_{\"cropped\" if facescrub_args[\"cropped\"] else \"uncropped\"}.pickle'\n",
    "            with open(before_idia_results_file_name, 'rb') as f:\n",
    "                tpr_before_cr_on_all_ids, fpr_before_cr_on_all_ids, result_dict_before_cr = pickle.load(f)\n",
    "\n",
    "            results = pd.Series(result_dict_before_cr).to_frame().rename(columns={0: 'before'})\n",
    "            results['after'] = pd.Series(result_dict)\n",
    "            names_not_to_be_unlearned_df = results[~results.index.isin(names_to_be_removed_by_seed[seed][:ids_removed])]\n",
    "            names_to_be_unlearned_df = results[results.index.isin(names_to_be_removed_by_seed[seed][:ids_removed])]\n",
    "\n",
    "            wrongfully_unlearned_names = names_not_to_be_unlearned_df[\n",
    "                (names_not_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_not_to_be_unlearned_df['after'] < cfg.idia.min_num_correct_prompt_preds)]\n",
    "            not_unlearned_names = names_not_to_be_unlearned_df[\n",
    "                (names_not_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds) &\n",
    "                (names_not_to_be_unlearned_df['after'] >= cfg.idia.min_num_correct_prompt_preds) |\n",
    "                (names_not_to_be_unlearned_df['before'] < cfg.idia.min_num_correct_prompt_preds) &\n",
    "                (names_not_to_be_unlearned_df['after'] < cfg.idia.min_num_correct_prompt_preds)]\n",
    "            newly_recalled_names = names_not_to_be_unlearned_df[\n",
    "                (names_not_to_be_unlearned_df['before'] < cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_not_to_be_unlearned_df['after'] >= cfg.idia.min_num_correct_prompt_preds)]\n",
    "            correctly_unlearned_names = names_to_be_unlearned_df[\n",
    "                (names_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_to_be_unlearned_df['after'] < cfg.idia.min_num_correct_prompt_preds)]\n",
    "            failed_unlearned_names = names_to_be_unlearned_df[\n",
    "                (names_to_be_unlearned_df['before'] >= cfg.idia.min_num_correct_prompt_preds)\n",
    "                & (names_to_be_unlearned_df['after'] >= cfg.idia.min_num_correct_prompt_preds)]\n",
    "            \n",
    "            result_dict = {\n",
    "                'wrongfully_unlearned_names': len(wrongfully_unlearned_names),\n",
    "                'wrongfully_unlearned_names_perc': 100 * len(wrongfully_unlearned_names) / len(names_not_to_be_unlearned_df),\n",
    "                'not_unlearned_names': len(not_unlearned_names),\n",
    "                'not_unlearned_names_perc': 100 * len(not_unlearned_names) / len(names_not_to_be_unlearned_df),\n",
    "                'newly_recalled_names': len(newly_recalled_names),\n",
    "                'newly_recalled_names_perc': 100 * len(newly_recalled_names) / len(names_not_to_be_unlearned_df),\n",
    "                'correctly_unlearned_names': len(correctly_unlearned_names),\n",
    "                'correctly_unlearned_names_perc': 100 * len(correctly_unlearned_names) / len(names_to_be_unlearned_df),\n",
    "                'failed_unlearned_names': len(failed_unlearned_names),\n",
    "                'failed_unlearned_names_perc': 100 * len(failed_unlearned_names) / len(names_to_be_unlearned_df),\n",
    "                'top1': top1,\n",
    "                'top5': top5\n",
    "            }\n",
    "\n",
    "            results_per_seed.append(result_dict)\n",
    "\n",
    "        print(f'Adding {key} {ids_removed}')\n",
    "        idia_result_dict[key][ids_removed] = results_per_seed\n",
    "\n",
    "\n",
    "        with open('./merging_experiment_results_new.pickle', 'wb') as f:\n",
    "            pickle.dump(idia_result_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('./merging_experiment_results_new.pickle', 'rb') as f:\n",
    "    merging_experiment_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_dict = {}\n",
    "\n",
    "for key in merging_experiment_results.keys():\n",
    "    metrics_dict = {\n",
    "        'idx': [],\n",
    "        'top1': [],\n",
    "        'top5': [],\n",
    "        'correctly_unlearned_names': [], \n",
    "        'correctly_unlearned_names_perc': [], \n",
    "        'newly_recalled_names': [], \n",
    "        'newly_recalled_names_perc': [], \n",
    "        'wrongfully_unlearned_names': [], \n",
    "        'wrongfully_unlearned_names_perc': [],\n",
    "        'failed_unlearned_names': [],\n",
    "        'failed_unlearned_names_perc': []\n",
    "    }\n",
    "    for num_ids_removed in merging_experiment_results[key].keys():\n",
    "        for res in merging_experiment_results[key][num_ids_removed]:\n",
    "            res['idx'] = num_ids_removed\n",
    "            for metrics_key in metrics_dict.keys():\n",
    "                metrics_dict[metrics_key].append(res[metrics_key])\n",
    "\n",
    "    metrics_df_dict[key] = {\n",
    "        'df': pd.DataFrame(metrics_dict),\n",
    "        'plot_x_label': 'Number of Removed Identities'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = metrics_df_dict['no_wl_vitb32']['df']\n",
    "test[test['idx'] == 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imagenet_weight_reg(df_with_wl, df_without_wl, filename, x_label, top1_baseline, top5_baseline, num_poisoned_samples=False, y_axis_label=True, x_axis_label=True, legend=True):  \n",
    "    df_with = df_with_wl['df']\n",
    "    df_with['weight_reg'] = 'w/ Weight Reg.'\n",
    "    df_without = df_without_wl['df']\n",
    "    df_without['weight_reg'] = 'w/o Weight Reg.'\n",
    "    df = pd.concat([df_with, df_without], axis=0).reset_index()\n",
    "    metrics_df = df[['idx', 'top1', 'top5', 'weight_reg']].rename(columns={'top1': 'Acc@1', 'top5': 'Acc@5'}).melt(id_vars=['idx', 'weight_reg'])\n",
    "    metrics_df[x_label] = np.log2(metrics_df['idx']) + 1\n",
    "    metrics_df = metrics_df.rename(columns={'value': 'Accuracy in %'})\n",
    "    plt.clf()\n",
    "    sn.set_style(\"whitegrid\")\n",
    "    ax = sn.lineplot(metrics_df, x=x_label, y='Accuracy in %', style='weight_reg', hue=\"variable\", markers=True)\n",
    "\n",
    "    ax.set_ylim(top1_baseline-15, top5_baseline+5)\n",
    "    if not x_axis_label:\n",
    "        ax.set_xticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_xticklabels([0] + [int(2 ** (x-1) * (4 if num_poisoned_samples else 1)) for x in metrics_df[x_label].unique()], size=16)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "    if not y_axis_label:\n",
    "        ax.set_yticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "    for label in ax.yaxis.get_ticklabels()[0::2]:\n",
    "        label.set_visible(False)\n",
    "    ax.axhline(y=top1_baseline, linewidth=2, color='gray', ls='dashed')\n",
    "    ax.axhline(y=top5_baseline, linewidth=2, color='gray', ls='dashed')\n",
    "    ax.get_figure().set_figwidth(7)\n",
    "    for line in ax.lines:\n",
    "        line.set_markersize(12)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles.append(\n",
    "        Line2D([0], [0], label='Clean\\nBaseline', markersize=10, color='gray', linestyle='dashed', linewidth=2)\n",
    "    )\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    handles = handles[:2] + handles[3:]\n",
    "    # labels = labels[:3] + labels[3:]\n",
    "\n",
    "    ax.legend(handles=handles, loc='lower left', bbox_to_anchor=(-0.01, 0.32), ncol=2, title=None, fontsize=16, columnspacing=0.5)\n",
    "    if not legend:\n",
    "        ax.get_legend().remove()\n",
    "    \n",
    "    if not x_axis_label:\n",
    "        ax.set(xlabel=None)\n",
    "    else:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "    if not y_axis_label:\n",
    "        ax.set(ylabel=None)\n",
    "    else:\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_imagenet.pdf')\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_imagenet.png')\n",
    "\n",
    "x_label = 'Number of Removed Identities'\n",
    "\n",
    "plot_imagenet_weight_reg(\n",
    "    metrics_df_dict['with_wl_vitb32'].copy(), \n",
    "    metrics_df_dict['no_wl_vitb32'].copy(), \n",
    "    'vitb32', \n",
    "    x_label, \n",
    "    top1_baseline=52.459, \n",
    "    top5_baseline=79.4,\n",
    "    x_axis_label=True,\n",
    "    y_axis_label=False,\n",
    "    legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics(df, filename, x_label, num_poisoned_samples=False):\n",
    "#     sn.set(rc={'text.usetex' : True})\n",
    "#     plt.clf()\n",
    "#     metrics_df = df[['idx', 'failed_unlearned_names_perc', 'correctly_unlearned_names_perc']].rename(columns={'failed_unlearned_names_perc': 'IDIA TPR', 'correctly_unlearned_names_perc': 'IDIA FNR'}).melt('idx')\n",
    "#     metrics_df[x_label] = np.log2(metrics_df['idx']) + 1\n",
    "#     metrics_df = metrics_df.rename(columns={'value': 'Value'})\n",
    "#     ax.set_xticklabels([0] + [int(2 ** (x-1) * (4 if num_poisoned_samples else 1)) for x in metrics_df[x_label].unique()], size=16, weight='bold')\n",
    "#     ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "#     ax.set_yticklabels(ax.get_yticklabels(), size=16, weight='bold')\n",
    "#     for label in ax.yaxis.get_ticklabels()[0::2]:\n",
    "#         label.set_visible(False)\n",
    "#     ax.get_figure().set_figwidth(7)\n",
    "#     ax.legend(loc='lower left', bbox_to_anchor=(0.2, 0.4, 0, 0), ncol=2, title=None, markerscale=1.5, fontsize=16)\n",
    "#     for line in ax.lines:\n",
    "#         line.set_markersize(10)\n",
    "#     ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "#     ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "#     plt.subplots_adjust(bottom=0.15)\n",
    "#     ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_metrics.pdf')\n",
    "#     ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_metrics.png')\n",
    "#     sn.set(rc={'text.usetex' : False})\n",
    "\n",
    "\n",
    "def plot_metrics(df, filename, x_label, num_poisoned_samples=False, tpr_baseline=1, y_axis_label=True, x_axis_label=True, legend=True):\n",
    "    # sn.set(rc={'text.usetex' : True})\n",
    "    plt.clf()\n",
    "    metrics_df = df[['idx', 'failed_unlearned_names_perc', 'correctly_unlearned_names_perc']].rename(columns={'failed_unlearned_names_perc': 'IDIA TPR', 'correctly_unlearned_names_perc': 'IDIA FNR'}).melt('idx')\n",
    "    metrics_df[x_label] = np.log2(metrics_df['idx']) + 1\n",
    "    metrics_df = metrics_df.rename(columns={'value': 'Value'})\n",
    "    metrics_df\n",
    "\n",
    "    sn.set_style(\"whitegrid\")\n",
    "    ax = sn.lineplot(metrics_df, x=x_label, y='Value', style='variable', hue=\"variable\", markers=True, linewidth=2)\n",
    "\n",
    "    # add the tpr baseline\n",
    "    ax.axhline(y=tpr_baseline, color='gray', linewidth=2, ls='dashed', label='IDIA TPR w/o Defense')\n",
    "\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    if not x_axis_label:\n",
    "        ax.set_xticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_xticklabels([0] + [int(2 ** (x-1) * (4 if num_poisoned_samples else 1)) for x in metrics_df[x_label].unique()], size=16)\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "    \n",
    "    if not y_axis_label:\n",
    "        ax.set_yticklabels([], size=16)\n",
    "    else:\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), size=16)\n",
    "\n",
    "    for label in ax.yaxis.get_ticklabels()[0::2]:\n",
    "        label.set_visible(False)\n",
    "    ax.get_figure().set_figwidth(7)\n",
    "    ax.legend(loc='lower left', bbox_to_anchor=(0., 0.3, 0, 0), ncol=1, title=None, markerscale=2, fontsize=18, columnspacing=-4)\n",
    "    if not legend:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    for line in ax.lines:\n",
    "        line.set_markersize(12)\n",
    "    if not x_axis_label:\n",
    "        ax.set(xlabel=None)   \n",
    "    else:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=18)\n",
    "    if not y_axis_label:\n",
    "        ax.set(ylabel=None)\n",
    "    else:\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=18)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_metrics.pdf')\n",
    "    ax.get_figure().savefig(f'./plots/merge_encoder/{filename}_metrics.png')\n",
    "    # sn.set(rc={'text.usetex' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, values in metrics_df_dict.items():\n",
    "#     df = values['df'].copy()\n",
    "\n",
    "#     df['failed_unlearned_names_perc'] = df['failed_unlearned_names_perc'] / 100\n",
    "#     df['correctly_unlearned_names_perc'] = df['correctly_unlearned_names_perc'] / 100\n",
    "    \n",
    "#     # plot_imagenet(df, key, values['plot_x_label'], num_poisoned_samples='Poisoned' in values['plot_x_label'])\n",
    "#     plot_metrics(df, key, values['plot_x_label'], num_poisoned_samples='Poisoned' in values['plot_x_label'])\n",
    "\n",
    "df = metrics_df_dict['with_wl_vitb32']['df'].copy()\n",
    "df['failed_unlearned_names_perc'] = df['failed_unlearned_names_perc'] / 100\n",
    "df['correctly_unlearned_names_perc'] = df['correctly_unlearned_names_perc'] / 100\n",
    "plot_metrics(df, key, metrics_df_dict['with_wl_vitb32']['plot_x_label'], num_poisoned_samples='Poisoned' in metrics_df_dict['with_wl_vitb32']['plot_x_label'], legend=False, x_axis_label=False, y_axis_label=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
